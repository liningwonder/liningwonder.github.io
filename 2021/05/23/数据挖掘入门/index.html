<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"localhost","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="数据挖掘流程 十大算法 分类算法 聚类算法 关联分析 连接分析 预测   数学知识 编程语言 商业智能 BI、数据仓库 DW、数据挖掘 DM 用户画像 - 精细化运营 用户唯一标识是整个用户画像的核心。 其次，给用户打标签。   决策树（分类和回归） 构造和剪枝 纯度和信息熵- 信息熵越大，纯度越低 ID3 算法 C4.5 算法 CART算法 分类 回归     朴素贝叶斯 支持向量机 KN">
<meta property="og:type" content="article">
<meta property="og:title" content="数据挖掘入门">
<meta property="og:url" content="http://localhost:4000/2021/05/23/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="李宁的博客">
<meta property="og:description" content="数据挖掘流程 十大算法 分类算法 聚类算法 关联分析 连接分析 预测   数学知识 编程语言 商业智能 BI、数据仓库 DW、数据挖掘 DM 用户画像 - 精细化运营 用户唯一标识是整个用户画像的核心。 其次，给用户打标签。   决策树（分类和回归） 构造和剪枝 纯度和信息熵- 信息熵越大，纯度越低 ID3 算法 C4.5 算法 CART算法 分类 回归     朴素贝叶斯 支持向量机 KN">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://localhost:4000/images/s.webp">
<meta property="og:image" content="http://localhost:4000/images/sa.webp">
<meta property="article:published_time" content="2021-05-22T16:00:00.000Z">
<meta property="article:modified_time" content="2021-07-08T08:01:57.619Z">
<meta property="article:author" content="李宁">
<meta property="article:tag" content="数据挖掘">
<meta property="article:tag" content="分类算法">
<meta property="article:tag" content="聚类算法">
<meta property="article:tag" content="关联分析">
<meta property="article:tag" content="连接分析">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://localhost:4000/images/s.webp">

<link rel="canonical" href="http://localhost:4000/2021/05/23/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%85%A5%E9%97%A8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>数据挖掘入门 | 李宁的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">李宁的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://localhost:4000/2021/05/23/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李宁">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李宁的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          数据挖掘入门
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-23 00:00:00" itemprop="dateCreated datePublished" datetime="2021-05-23T00:00:00+08:00">2021-05-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-08 16:01:57" itemprop="dateModified" datetime="2021-07-08T16:01:57+08:00">2021-07-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" itemprop="url" rel="index"><span itemprop="name">数据挖掘</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <!-- toc -->

<ul>
<li><a href="#数据挖掘流程">数据挖掘流程</a></li>
<li><a href="#十大算法">十大算法</a><ul>
<li><a href="#分类算法">分类算法</a></li>
<li><a href="#聚类算法">聚类算法</a></li>
<li><a href="#关联分析">关联分析</a></li>
<li><a href="#连接分析">连接分析</a></li>
<li><a href="#预测">预测</a></li>
</ul>
</li>
<li><a href="#数学知识">数学知识</a></li>
<li><a href="#编程语言">编程语言</a></li>
<li><a href="#商业智能-bi-数据仓库-dw-数据挖掘-dm">商业智能 BI、数据仓库 DW、数据挖掘 DM</a></li>
<li><a href="#用户画像-精细化运营">用户画像 - 精细化运营</a><ul>
<li><a href="#用户唯一标识是整个用户画像的核心">用户唯一标识是整个用户画像的核心。</a></li>
<li><a href="#其次给用户打标签">其次，给用户打标签。</a></li>
</ul>
</li>
<li><a href="#决策树分类和回归">决策树（分类和回归）</a><ul>
<li><a href="#构造和剪枝">构造和剪枝</a></li>
<li><a href="#纯度和信息熵-信息熵越大纯度越低">纯度和信息熵- 信息熵越大，纯度越低</a></li>
<li><a href="#id3-算法">ID3 算法</a></li>
<li><a href="#c45-算法">C4.5 算法</a></li>
<li><a href="#cart算法">CART算法</a><ul>
<li><a href="#分类">分类</a></li>
<li><a href="#回归">回归</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#朴素贝叶斯">朴素贝叶斯</a></li>
<li><a href="#支持向量机">支持向量机</a></li>
<li><a href="#knn">KNN</a></li>
</ul>
<!-- tocstop -->

<hr>
<h2><span id="数据挖掘流程">数据挖掘流程</span></h2><ul>
<li>商业理解</li>
</ul>
<ul>
<li>数据理解</li>
</ul>
<ul>
<li>数据准备</li>
</ul>
<p>​       数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换。</p>
<p>​       数据清洗：主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。</p>
<p>​       数据集成：是将多个数据源中的数据存放在一个统一的数据存储中。</p>
<p>​       数据变换：就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 0~1 之间。</p>
<ul>
<li>模型建立</li>
</ul>
<ul>
<li>模型评估</li>
</ul>
<ul>
<li>上线发布</li>
</ul>
<h2><span id="十大算法">十大算法</span></h2><h3><span id="分类算法">分类算法</span></h3><hr>
<p>就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类。</p>
<ul>
<li>C4.5 算法</li>
</ul>
<p>​       C4.5 是决策树的算法，</p>
<ul>
<li>朴素贝叶斯（Naive Bayes）</li>
</ul>
<p>​       朴素贝叶斯模型是基于概率论的原理</p>
<ul>
<li>支持向量机（SVM - Support Vector Machine）</li>
</ul>
<p>​       VM 在训练中建立了一个超平面的分类模型</p>
<ul>
<li>K 最近邻算法(KNN - K-Nearest Neighbor)</li>
</ul>
<p>​       就是每个样本都可以用它最接近的 K 个邻居来代表</p>
<ul>
<li>Adaboost</li>
</ul>
<p>​       Adaboost 在训练中建立了一个联合的分类模型</p>
<ul>
<li>CART - Classification and Regression Trees</li>
</ul>
<p>​       CART 代表分类和回归树</p>
<h3><span id="聚类算法">聚类算法</span></h3><hr>
<p>聚类就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大</p>
<ul>
<li>K-Means</li>
</ul>
<p>K-Means 算法是一个聚类算法</p>
<ul>
<li>最大似然估计</li>
</ul>
<p>EM 算法也叫最大期望算法，EM 算法经常用于聚类和机器学习领域中。</p>
<h3><span id="关联分析">关联分析</span></h3><hr>
<p>就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中。</p>
<ul>
<li>Apriori </li>
</ul>
<p>Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。常用于推荐系统。</p>
<h3><span id="连接分析">连接分析</span></h3><hr>
<ul>
<li>PageRank</li>
</ul>
<h3><span id="预测">预测</span></h3><hr>
<h2><span id="数学知识">数学知识</span></h2><p>概率论与数理统计、线性代数、图论、最优化方法</p>
<h2><span id="编程语言">编程语言</span></h2><p>毫无疑问选择Python，并了解相关得库NumPy、Pandas</p>
<h2><span id="商业智能-bi-数据仓库-dw-数据挖掘-dm">商业智能 BI、数据仓库 DW、数据挖掘 DM</span></h2><p> 三者之间的关系开头中的百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在数据仓库中，通过对个体进行消费行为分析总结出来的规律属于数据挖掘。</p>
<h2><span id="用户画像-精细化运营">用户画像 - 精细化运营</span></h2><h3><span id="用户唯一标识是整个用户画像的核心">用户唯一标识是整个用户画像的核心。</span></h3><hr>
<p>设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等，且后续数据仓库需要做ID-Mapping来关联这些ID.</p>
<h3><span id="其次给用户打标签">其次，给用户打标签。</span></h3><hr>
<p>可以说，用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。以用户消费行为分析为例，从这 4 个维度来进行标签划分。</p>
<p>用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。</p>
<p>消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。</p>
<p>行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。</p>
<p>内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。</p>
<p>最后，当你有了用户画像，可以为企业带来什么业务价值呢？我们可以从用户生命周期的三个阶段来划分业务价值，包括：获客、粘客和留客。</p>
<p>获客：如何进行拉新，通过更精准的营销获取客户。</p>
<p>粘客：个性化推荐，搜索排序，场景运营等。</p>
<p>留客：流失率预测，分析关键节点降低流失率。 留存率</p>
<p>如果按照数据流处理的阶段来划分用户画像建模的过程，可以分为数据层、算法层和业务层。你会发现在不同的层，都需要打上不同的标签。数据层指的是用户消费行为里的标签。我们可以打上“事实标签”，作为数据客观的记录。算法层指的是透过这些行为算出的用户建模。我们可以打上“模型标签”，作为用户画像的分类标识。业务层指的是获客、粘客、留客的手段。我们可以打上“预测标签”，作为业务关联的结果。</p>
<p>所以这个标签化的流程，就是通过数据层的“事实标签”，在算法层进行计算，打上“模型标签”的分类结果，最后指导业务层，得出“预测标签”。</p>
<p>以美团外卖为例：</p>
<p>用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。</p>
<p>消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。</p>
<p>行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。</p>
<p>内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。</p>
<h2><span id="决策树分类和回归">决策树（分类和回归）</span></h2><p>决策树基本上就是把我们以前的经验总结出来。我给你准备了一个打篮球的训练集。如果我们要出门打篮球，一般会根据“天气”、“温度”、“湿度”、“刮风”这几个条件来判断，最后得到结果：去打篮球？还是不去？</p>
<h3><span id="构造和剪枝">构造和剪枝</span></h3><hr>
<p>构造</p>
<p>构造的过程就是选择什么属性作为节点的过程。节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：选择哪个属性作为根节点；选择哪些属性作为子节点；什么时候停止并得到目标状态，即叶节点。</p>
<p>剪枝</p>
<p>剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止“过拟合”（Overfitting）现象的发生。</p>
<p>“过拟合”这个概念你一定要理解，它指的就是模型的训练结果“太好了”，以至于在实际应用的过程中，会存在“死板”的情况，导致分类错误。造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。</p>
<p>欠拟合，和过拟合就好比是下面这张图中的第一个和第三个情况一样，训练的结果“太好“，反而在实际应用过程中会导致分类错误。</p>
<p>预剪枝</p>
<p>预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。</p>
<p>后剪枝</p>
<p>后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。</p>
<h3><span id="纯度和信息熵-信息熵越大纯度越低">纯度和信息熵- 信息熵越大，纯度越低</span></h3><hr>
<p>D3 算法，基于信息增益做判断；</p>
<p>C4.5 算法，基于信息增益率做判断；</p>
<p>CART 算法，分类树是基于基尼系数做判断。回归树是基于偏差做判断。</p>
<p>我在这里举个例子，假设有 3 个集合：集合 1：6 次都去打篮球；集合 2：4 次去打篮球，2 次不去打篮球；集合 3：3 次去打篮球，3 次不去打篮球。按照纯度指标来说，集合 1&gt; 集合 2&gt; 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。</p>
<p>为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。概率大，出现机会多，不确定性小；反之不确定性就大。</p>
<img src="/images/s.webp" width="74%" height="75%">



<p>在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。那么假设：类别 1 为“打篮球”，即次数为 5；类别 2 为“不打篮球”，即次数为 1。那么节点划分为类别 1 的概率是 5/6，为类别 2 的概率是 1/6，带入上述信息熵公式可以计算得出</p>
<p>从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。</p>
<p>我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。</p>
<h3><span id="id3-算法">ID3 算法</span></h3><hr>
<p>ID3 算法计算的是信息增益，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。</p>
<p>它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。</p>
<img src="/images/sa.webp" width="74%" height="75%">

<p>在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。于是我们通过 ID3 算法得到了一棵决策树。ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 ID3 算法倾向于选择取值比较多的属性。这样，如果我们把“编号”作为一个属性（一般情况下不会这么做，这里只是举个例子），那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对“打篮球”的分类并没有太大作用。</p>
<h3><span id="c45-算法">C4.5 算法</span></h3><hr>
<p>在 ID3 算法上进行改进的 C4.5 算法.</p>
<p>因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。</p>
<p>信息增益率 = 信息增益 / 属性熵.</p>
<p>1.采用信息增益率</p>
<p>采用信息增益率因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵，具体的计算公式这里省略。当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。</p>
<p>2.采用悲观剪枝</p>
<p>​        ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛        化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。</p>
<p>3.离散化处理连续属性</p>
<p>C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。</p>
<p>4.处理缺失值</p>
<p>C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。</p>
<h3><span id="cart算法">CART算法</span></h3><hr>
<p>CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。</p>
<p>我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。</p>
<h4><span id="分类">分类</span></h4><p>分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，</p>
<h4><span id="回归">回归</span></h4><p>而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p>
<p>实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。</p>
<p>基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。</p>
<p>CART 分类树实际上是基于基尼系数来做属性划分的。在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># encoding&#x3D;utf-8</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line"># 准备数据集</span><br><span class="line">iris&#x3D;load_iris()</span><br><span class="line"># 获取特征集和分类标识</span><br><span class="line">features &#x3D; iris.data</span><br><span class="line">labels &#x3D; iris.target</span><br><span class="line"># 随机抽取33%的数据作为测试集，其余为训练集</span><br><span class="line">train_features, test_features, train_labels, test_labels &#x3D; train_test_split(features, labels, test_size&#x3D;0.33, random_state&#x3D;0)</span><br><span class="line"># 创建CART分类树</span><br><span class="line">clf &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;gini&#39;)</span><br><span class="line"># 拟合构造CART分类树</span><br><span class="line">clf &#x3D; clf.fit(train_features, train_labels)</span><br><span class="line"># 用CART分类树做预测</span><br><span class="line">test_predict &#x3D; clf.predict(test_features)</span><br><span class="line"># 预测结果与测试集结果作比对</span><br><span class="line">score &#x3D; accuracy_score(test_labels, test_predict)</span><br><span class="line">print(&quot;CART分类树准确率 %.4lf&quot; % score)</span><br></pre></td></tr></table></figure>



<p>CART 决策树的剪枝主要采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：</p>
<p>到目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 gini：entropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；gini：默认参数，基于基尼系数。CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。</p>
<p>基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。</p>
<h2><span id="朴素贝叶斯">朴素贝叶斯</span></h2><p>先验概率</p>
<p>通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。</p>
<p>后验概率</p>
<p>后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。</p>
<p>条件概率</p>
<p>事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。</p>
<p>似然函数</p>
<p>实际上贝叶斯原理就是求解后验概率</p>
<p>朴素贝叶斯</p>
<p>它是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。</p>
<p>为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。</p>
<p>朴素贝叶斯分类是常用的贝叶斯分类方法。我们日常生活中看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。</p>
<p>朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。</p>
<p>sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。</p>
<p>高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。</p>
<p>多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。</p>
<p>伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。</p>
<p>词的 TF-IDF 值</p>
<p>TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。词频 TF 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。逆向文档频率 IDF，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。</p>
<p>TF-IDF=TF*IDF</p>
<h2><span id="支持向量机">支持向量机</span></h2><p>SVM 的英文叫 Support Vector Machine，中文名为支持向量机。它是常见的一种分类方法，在机器学习中，SVM 是有监督的学习模型。什么是有监督的学习模型呢？它指的是我们需要事先对数据打上分类标签，这样机器就知道这个数据属于哪个分类。同样无监督学习，就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代我们部分完成这个工作，比如将数据进行聚类，方便后续人工对每个类进行分析。SVM 作为有监督的学习模型，通常可以帮我们模式识别、分类以及回归分析。</p>
<p>在这里，二维平面变成了三维空间。原来的曲线变成了一个平面。这个平面，我们就叫做超平面。</p>
<p>用 SVM 计算的过程就是帮我们找到那个超平面的过程，这个超平面就是我们的 SVM 分类器。</p>
<p>实际上，我们的分类环境不是在二维平面中的，而是在多维空间中</p>
<p>SVM 就是帮我们找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔，支持向量）最大化。</p>
<p>假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。</p>
<p>另外还存在一种情况，就是非线性支持向量机。比如下面的样本集就是个非线性的数据。图中的两类数据，分别分布为两个圆圈的形状。那么这种情况下，不论是多高级的分类器，只要映射函数是线性的，就没法处理，SVM 也处理不了。这时，我们需要引入一个新的概念：核函数。它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。所以在非线性 SVM 中，核函数的选择就是影响 SVM 最大的变量。最常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid 核，或者是这些核函数的组合。这些函数的区别在于映射方式的不同。通过这些核函数，我们就可以把样本空间投射到新的高维空间中。</p>
<p>SVM 本身是一个二值分类器，最初是为二分类问题设计的，也就是回答 Yes 或者是 No。而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。</p>
<p>SVM 是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。</p>
<p>在 Python 的 sklearn 工具包中有 SVM 算法，首先需要引用工具包：</p>
<p>SVM 既可以做回归，也可以做分类器。当用 SVM 做回归的时候，我们可以使用 SVR 或 LinearSVR。SVR 的英文是 Support Vector Regression。这篇文章只讲分类，这里只是简单地提一下。当做分类器的时候，我们使用的是 SVC 或者 LinearSVC。SVC 的英文是 Support Vector Classification。</p>
<p>我们首先使用 SVC 的构造函数：model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)，这里有三个重要的参数 kernel、C 和 gamma。kernel 代表核函数的选择，它有四种选择，只不过默认是 rbf，即高斯核函数。linear：线性核函数poly：多项式核函数rbf：高斯核函数（默认）sigmoid：sigmoid 核函数</p>
<p>线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。了解深度学习的同学应该知道 sigmoid 经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from sklearn import svm</span><br><span class="line"></span><br><span class="line"># 加载数据集，你需要把数据放到目录中</span><br><span class="line">data &#x3D; pd.read_csv(&quot;.&#x2F;data.csv&quot;)</span><br><span class="line"># 数据探索</span><br><span class="line"># 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来</span><br><span class="line">pd.set_option(&#39;display.max_columns&#39;, None)</span><br><span class="line">print(data.columns)</span><br><span class="line">print(data.head(5))</span><br><span class="line">print(data.describe())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将特征字段分成3组</span><br><span class="line">features_mean&#x3D; list(data.columns[2:12])</span><br><span class="line">features_se&#x3D; list(data.columns[12:22])</span><br><span class="line">features_worst&#x3D;list(data.columns[22:32])</span><br><span class="line"># 数据清洗</span><br><span class="line"># ID列没有用，删除该列</span><br><span class="line">data.drop(&quot;id&quot;,axis&#x3D;1,inplace&#x3D;True)</span><br><span class="line"># 将B良性替换为0，M恶性替换为1</span><br><span class="line">data[&#39;diagnosis&#39;]&#x3D;data[&#39;diagnosis&#39;].map(&#123;&#39;M&#39;:1,&#39;B&#39;:0&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 特征选择</span><br><span class="line">features_remain &#x3D; [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;] </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 抽取30%的数据作为测试集，其余作为训练集</span><br><span class="line">train, test &#x3D; train_test_split(data, test_size &#x3D; 0.3)# in this our main data is splitted into train and test</span><br><span class="line"># 抽取特征选择的数值作为训练和测试数据</span><br><span class="line">train_X &#x3D; train[features_remain]</span><br><span class="line">train_y&#x3D;train[&#39;diagnosis&#39;]</span><br><span class="line">test_X&#x3D; test[features_remain]</span><br><span class="line">test_y &#x3D;test[&#39;diagnosis&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1</span><br><span class="line">ss &#x3D; StandardScaler()</span><br><span class="line">train_X &#x3D; ss.fit_transform(train_X)</span><br><span class="line">test_X &#x3D; ss.transform(test_X)</span><br></pre></td></tr></table></figure>



<h2><span id="knn">KNN</span></h2><p>KNN中K 值的选择还是很重要的，如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 K 值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为 K 值。</p>
<p>在 KNN 算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。</p>
<p>关于距离的计算方式有下面五种方式：欧氏距离；曼哈顿距离；闵可夫斯基距离；切比雪夫距离；余弦距离。</p>
<p>其实从上文你也能看出来，KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。</p>
<p>KNN 不仅可以做分类，还可以做回归。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。</p>
<p>同样 KNN 也可以用于推荐算法，虽然现在很多推荐系统的算法会使用 TD-IDF、协同过滤、Apriori 算法，不过针对数据量不大的情况下，采用 KNN 作为推荐算法也是可行的</p>
<p>在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。如果是做分类，你需要引用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br></pre></td></tr></table></figure>

<p>从名字上你也能看出来 Classifier 对应的是分类，Regressor 对应的是回归。一般来说如果一个算法有 Classifier 类，都能找到相应的 Regressor 类。比如在决策树分类中，你可以使用 DecisionTreeClassifier，也可以使用决策树来做回归 DecisionTreeRegressor。好了，我们看下如何在 sklearn 中创建 KNN 分类器。这里，我们使用构造函数 KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数，我分别来讲解下：1.n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。2.weights：是用来确定邻居的权重，有三种方式：weights=uniform，代表所有邻居的权重相同；weights=distance，代表权重是距离的倒数，即与距离成反比；自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果 0-9 一一对应起来。完整的手写数字数据集 MNIST 里面包括了 60000 个训练样本，以及 10000 个测试样本。如果你学习深度学习的话，MNIST 基本上是你接触的第一个数据集。今天我们用 sklearn 自带的手写数字数据集做 KNN 分类，你可以把这个数据集理解成一个简版的 MNIST 数据集，它只包括了 1797 幅数字图像，每幅图像大小是 8<em>8 像素。好了，我们先来规划下整个 KNN 分类的流程：整个训练过程基本上都会包括三个阶段：数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个 8</em>8 的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag"># 数据挖掘</a>
              <a href="/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" rel="tag"># 分类算法</a>
              <a href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/" rel="tag"># 聚类算法</a>
              <a href="/tags/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/" rel="tag"># 关联分析</a>
              <a href="/tags/%E8%BF%9E%E6%8E%A5%E5%88%86%E6%9E%90/" rel="tag"># 连接分析</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/25/%E5%A4%A7%E8%AF%9D%E5%AE%89%E5%85%A8/" rel="prev" title="IDaaS">
      <i class="fa fa-chevron-left"></i> IDaaS
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/06/25/redmine/" rel="next" title="Redmine项目管理工具">
      Redmine项目管理工具 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">1.</span> <span class="nav-text">数据挖掘流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">2.</span> <span class="nav-text">十大算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">2.1.</span> <span class="nav-text">分类算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">2.2.</span> <span class="nav-text">聚类算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">2.3.</span> <span class="nav-text">关联分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">2.4.</span> <span class="nav-text">连接分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">2.5.</span> <span class="nav-text">预测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">3.</span> <span class="nav-text">数学知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">4.</span> <span class="nav-text">编程语言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">5.</span> <span class="nav-text">商业智能 BI、数据仓库 DW、数据挖掘 DM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">6.</span> <span class="nav-text">用户画像 - 精细化运营</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">6.1.</span> <span class="nav-text">用户唯一标识是整个用户画像的核心。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">6.2.</span> <span class="nav-text">其次，给用户打标签。</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">7.</span> <span class="nav-text">决策树（分类和回归）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">7.1.</span> <span class="nav-text">构造和剪枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">7.2.</span> <span class="nav-text">纯度和信息熵- 信息熵越大，纯度越低</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">7.3.</span> <span class="nav-text">ID3 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">7.4.</span> <span class="nav-text">C4.5 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#"><span class="nav-number">7.5.</span> <span class="nav-text">CART算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#"><span class="nav-number">7.5.1.</span> <span class="nav-text">分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#"><span class="nav-number">7.5.2.</span> <span class="nav-text">回归</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">8.</span> <span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">9.</span> <span class="nav-text">支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number">10.</span> <span class="nav-text">KNN</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">李宁</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李宁</span>
</div>
  <div class="powered-by">万物之始，大道至简
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">知易行难，悟在天成
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
